{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a87b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = English()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)&(not t.is_stop)])\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def load_glove_embeddings(dolma):\n",
    "    vectorindex = {}\n",
    "    with open(dolma, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32') \n",
    "            vectorindex[word] = coefs # Taking the values corresponding to each word in Dolma .txt file and adding them to a dictionary\n",
    "    return vectorindex\n",
    "\n",
    "def listingvectorizer(listing, vectorindex, vector_size):\n",
    "    word_vectors = []\n",
    "    tokenized = simple_tokenizer(lemmatize(listing), model=en)\n",
    "    for word in tokenized:\n",
    "        if word in vectorindex:\n",
    "            word_vectors.append(vectorindex[word])\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0) # Finds the \"average\" of all the word vectors in a given document\n",
    "    else:\n",
    "        return np.zeros(vector_size) # If somehow no recognizable words at all, return zero vector\n",
    "\n",
    "def cosine_similarity(listing_x, listing_y):\n",
    "    dot_product = np.dot(listing_x, listing_y)\n",
    "    norm1 = np.linalg.norm(listing_x)\n",
    "    norm2 = np.linalg.norm(listing_y)\n",
    "    if norm1 == 0 or norm2 == 0: # If all words in a document are not recognized/do not exist\n",
    "        return 'NA'\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('dolma_300_2024_1.2M.100_combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a291d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "listingsim = []\n",
    "listvects = []\n",
    "\n",
    "INPUT = 'craigslistnonas.csv'\n",
    "\n",
    "df = pd.read_csv(INPUT)\n",
    "df.replace(np.nan, pd.NA, inplace=True)\n",
    "\n",
    "for row in df.iterrows():\n",
    "    listing = row[1]['descr']\n",
    "    listingvector = listingvectorizer(listing, glove_embeddings, vector_size=300) # Dim. of Dolma set to 300\n",
    "    listvects.append(listingvector)\n",
    "\n",
    "df['listingvector'] = listvects\n",
    "\n",
    "# Cartesian Product\n",
    "\n",
    "largecartesian = df.merge(df, how='cross')\n",
    "largecartesian = largecartesian[largecartesian['post_id_x'] < largecartesian['post_id_y']]\n",
    "largecartesian = largecartesian[largecartesian['descr_x'] != largecartesian['descr_y']]\n",
    "\n",
    "for row in largecartesian.iterrows():\n",
    "    vector1, vector2 = row[1]['listingvector_x'], row[1]['listingvector_y']\n",
    "    listingsimilarity = cosine_similarity(vector1, vector2)\n",
    "    listingsim.append(listingsimilarity)    \n",
    "\n",
    "largecartesian.drop(columns=['listingvector_x','listingvector_y'], inplace=True)\n",
    "largecartesian['similarity_score'] = listingsim  \n",
    "largecartesian.to_csv('craigslistcartbaycos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07fda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=simple_tokenizer, ngram_range=(1, 3)) \n",
    "count_desc_vecs = cv.fit_transform(craigslist['descr'].apply(lambda x:str(x))).toarray()\n",
    "desc_count = dict(zip(cv.get_feature_names_out(), count_desc_vecs.sum(axis=0)))\n",
    "\n",
    "n = 50\n",
    "print(sorted(desc_count.items(), key=lambda x: x[1], reverse=True)[:n])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
